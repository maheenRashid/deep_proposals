require 'nn';
require 'loadcaffe';
require 'cutorch';
require 'cunn';
require 'optim'
require 'hdf5'
require 'image'
require 'gnuplot'
npy4th = require 'npy4th'
require 'data_neg_new';

function makeMyNetwork(torch_file)
	local vgg=torch.load(torch_file);
	vgg:add(nn.SpatialConvolution(512,512,1,1));
	vgg:add(nn.SpatialConvolution(512,128,5,5));

	local score_branch=nn.Sequential();
	score_branch:add(nn.View(128*10*10));
	score_branch:add(nn.Linear(128*10*10,512));
	score_branch:add(nn.ReLU());
	score_branch:add(nn.Dropout(0.5));
	score_branch:add(nn.Linear(512,1024));
	score_branch:add(nn.ReLU());
	score_branch:add(nn.Dropout(0.5));
	score_branch:add(nn.Linear(1024,1));

	local seg_branch=nn.Sequential();
	seg_branch:add(nn.View(128*10*10));
	seg_branch:add(nn.Linear(128*10*10,512));
	seg_branch:add(nn.Linear(512,56*56));
	seg_branch:add(nn.View(1,56,56));
	seg_branch:add(nn.SpatialUpSamplingNearest(4));

	local split_net=nn.ConcatTable();
	split_net:add(score_branch);
	split_net:add(seg_branch);

    local total_net = nn.Sequential();
    total_net:add(vgg);
    total_net:add(split_net);

	return total_net
end

function getLossSeg(x_seg,gt_mask)
	local loss=torch.mul(gt_mask,-1)
	-- print(loss:size())
	-- print(gt_mask:size())
	loss:cmul(x_seg)
	loss:exp();
	loss:add(1);
	loss:log();
    loss:div(56*56);
	loss=loss:sum()
	return loss;
end

function getLossScore(x_score,gt_label)
    local loss=torch.mul(gt_label,-1)
    loss:cmul(x_score)
    loss:exp();
    loss:add(1);
    loss:log();
    loss:div(32);
    loss=loss:sum()
    return loss;
end

function getLossSegD(x_seg,gt_mask)
	local dloss=torch.mul(gt_mask,-1);
	dloss:cmul(x_seg);
    -- print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:exp();
    -- print (torch.min(dloss)..' '..torch.max(dloss));
	local deno=torch.add(dloss,1);
    -- print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:cmul(gt_mask);
    -- print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:mul(-1);
    -- print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:cdiv(deno);
    -- print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:div(56*56);
    -- print (torch.min(dloss)..' '..torch.max(dloss));
	return dloss;
end

function getLossScoreD(x_score,gt_label)
	-- print (x_score)
	local dloss=torch.mul(gt_label,-1);
	dloss:cmul(x_score);
    dloss:exp();
    local deno=torch.add(dloss,1);
    dloss:cmul(gt_label);
    dloss:mul(-1);
    dloss:cdiv(deno);
    dloss:div(32);
    return dloss;
end


local torch_file='/disk2/januaryExperiments/vgg_16/vgg16_onlyConv.dat';
local deep_prop_file='/disk2/marchExperiments/deep_proposals/new_design/model_untrained.dat'

local out_dir_intermediate='/disk2/marchExperiments/deep_proposals/new_design/training_100_new_neg_switched/intermediate';
local out_dir_final='/disk2/marchExperiments/deep_proposals/new_design/training_100_new_neg_switched/final';

local out_file_net=out_dir_final..'/'..'model_all_final.dat';

local out_file_loss_seg=out_dir_final..'/'..'loss_all_final_seg.npy';
local out_file_loss_score=out_dir_final..'/'..'loss_all_final_score.npy';

local out_file_intermediate_pre = out_dir_intermediate..'/'..'model_all_';
local out_file_loss_intermediate_pre = out_dir_intermediate..'/'..'loss_all_';

local out_file_loss_plot_intermediate_score=out_dir_intermediate..'/'..'loss_iter_score.png';
local out_file_loss_plot_final_score=out_dir_final..'/'..'loss_iter_score.png';

local out_file_loss_plot_intermediate_seg=out_dir_intermediate..'/'..'loss_iter_seg.png';
local out_file_loss_plot_final_seg=out_dir_final..'/'..'loss_iter_seg.png';


-- local im=torch.Tensor(1,3,224,224);
-- local net=makeMyNetwork(torch_file);
-- print (net)
-- local out=net:forward(im);
-- print (out);
-- torch.save(deep_prop_file,net);

local opt = {}         
opt.optimization = 'sgd'

opt.batch_size=32;
opt.learningRate=0.0001;
opt.momentum=0.9
opt.weightDecay=0.00005;
opt.iterations=3000;
opt.saveAfter=1000;
opt.plotAfter=50;
opt.dispAfter=4;

cutorch.setDevice(1);

local optimState       
local optimMethod      

optimState = {
    learningRate = opt.learningRate,
    weightDecay = opt.weightDecay,
    momentum = opt.momentum,
}

optimMethod = optim.sgd



print ('loading network');
-- net = torch.load(torch_file);

net = torch.load(deep_prop_file);
-- net:get(2):get(2):add(nn.Tanh())
-- print(net);

print ('done loading network');
-- print (net:get(1));
print (net);

print ('making cuda');
net = net:cuda();
print ('done');

print ('loading params');
local parameters, gradParameters = net:getParameters()
print ('loading done');

td = data();
local counter = 0

local fevalScore = function(x)
    if x ~= parameters then
    parameters:copy(x)
    end
    
    td:getTrainingDataToyScore();    
    local batch_inputs = td.training_set_score.data:cuda();
    local batch_targets = td.training_set_score.label:cuda();
    
    gradParameters:zero()
    -- In order, these lines compute:
    -- 1. compute outputs (log probabilities) for each data point
    
    local batch_outputs_mid = net:get(1):forward(batch_inputs)
    local batch_outputs = net:get(2):get(1):forward(batch_outputs_mid)
    -- print (batch_outputs:size())

    -- batch_outputs=batch_outputs[1];
    -- print (batch_outputs);
    -- 2. compute the loss of these outputs, measured against the true labels in batch_target
    local batch_loss = getLossScore(batch_outputs, batch_targets)
    -- 3. compute the derivative of the loss wrt the outputs of the model
    local dloss_doutput = getLossScoreD(batch_outputs, batch_targets)
    
    -- print ('loss '..batch_loss..' dloss '..torch.sum(dloss_doutput))

    -- 4. use gradients to update weights, we'll understand this step more next week
    -- get(31):get(1):
    local gradInputs=net:get(2):get(1):backward(batch_outputs_mid, dloss_doutput)
    -- print (gradInputs:size())
    net:get(1):backward(batch_inputs, gradInputs)
    -- print (net:get(5))
    -- print (torch.sum(net:get(2):get(2):get(1).weight))
    -- print (torch.sum(net:get(1):get(29).weight))
    -- print (torch.sum(net:get(1):get(29).weight))
    -- optim expects us to return
    --     loss, (gradient of loss with respect to the weights that we're optimizing)
    return batch_loss, gradParameters
end


local fevalSeg = function(x)
    if x ~= parameters then
    parameters:copy(x)
    end
    
    td:getTrainingDataToy();    
    local batch_inputs = td.training_set_seg.data:cuda();
    local batch_targets = td.training_set_seg.label:cuda();

    gradParameters:zero()
    -- In order, these lines compute:
    -- 1. compute outputs (log probabilities) for each data point
    
    local batch_outputs_mid = net:get(1):forward(batch_inputs)
    -- print (batch_outputs_mid:size())
    local batch_outputs = net:get(2):get(2):forward(batch_outputs_mid)
    -- print (batch_outputs:size())

    -- batch_outputs=batch_outputs[1];
    -- print (batch_outputs);
    -- 2. compute the loss of these outputs, measured against the true labels in batch_target
    local batch_loss = getLossSeg(batch_outputs, batch_targets)
    -- 3. compute the derivative of the loss wrt the outputs of the model
    local dloss_doutput = getLossSegD(batch_outputs, batch_targets)
    
    -- print ('loss '..batch_loss..' dloss '..torch.sum(dloss_doutput))

    -- 4. use gradients to update weights, we'll understand this step more next week
    -- get(31):get(1):
    local gradInputs=net:get(2):get(2):backward(batch_outputs_mid, dloss_doutput):clone()
    -- print (gradInputs:size())
    net:get(1):backward(batch_inputs, gradInputs)
    -- print (net:get(5))
    -- print (torch.sum(net:get(2):get(1):get(3).weight))
    -- print (torch.sum(net:get(1):get(29).weight))
    -- optim expects us to return
    --     loss, (gradient of loss with respect to the weights that we're optimizing)
    return batch_loss, gradParameters
end


local losses_seg = {};
local losses_score = {};

for i=1,opt.iterations do
    
    if i%2==0 then
        local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
        -- print(string.format("minibatches processed: %6s, loss = %6.6f", i, minibatch_loss[1]))
        losses_score[#losses_score + 1] = minibatch_loss[1] -- append the new loss        
    else
        local _, minibatch_loss = optimMethod(fevalSeg,parameters, optimState)
        -- print(string.format("minibatches processed: %6s, loss = %6.6f", i, minibatch_loss[1]))
        losses_seg[#losses_seg + 1] = minibatch_loss[1] -- append the new loss

    end

    if i%opt.dispAfter==0 then
        print(string.format("minibatches processed: %6s, loss seg = %6.6f, loss score = %6.6f", i, 
            losses_seg[#losses_seg],losses_score[#losses_score]))
    end

    -- check if model needs to be saved. save it.
    -- also save losses
    if i%opt.saveAfter==0 then
        local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
        torch.save(out_file_intermediate,net);
        local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_score.npy';
        npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses_score))
        local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_seg.npy';
        npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses_seg))
    end
    
    -- check if losses need to be plotted. plot them.
    if i%opt.plotAfter==0 then
        
        gnuplot.pngfigure(out_file_loss_plot_intermediate_score)
        local iteration_score = torch.range( 1, #losses_score)
        gnuplot.plot({'Score Loss', iteration_score, torch.Tensor(losses_score),  '-'})
        gnuplot.xlabel('Iterations')
        gnuplot.ylabel('Loss')
        gnuplot.plotflush()

        gnuplot.pngfigure(out_file_loss_plot_intermediate_seg)
        local iteration_seg = torch.range( 1, #losses_seg)
        gnuplot.plot({'Segmentation Loss', iteration_seg, torch.Tensor(losses_seg), '-'})
        gnuplot.xlabel('Iterations')
        gnuplot.ylabel('Loss')
        gnuplot.plotflush()
    end

end

-- save final model
torch.save(out_file_net,net);
npy4th.savenpy(out_file_loss_seg, torch.Tensor(losses_seg))
npy4th.savenpy(out_file_loss_score, torch.Tensor(losses_score))

-- save final plot

gnuplot.pngfigure(out_file_loss_plot_final_score)
local iteration_score = torch.range( 1, #losses_score)
gnuplot.plot({'Score Loss', iteration_score, torch.Tensor(losses_score),  '-'})
gnuplot.xlabel('Iterations')
gnuplot.ylabel('Loss')
gnuplot.plotflush()

gnuplot.pngfigure(out_file_loss_plot_final_seg)
local iteration_seg = torch.range( 1, #losses_seg)
gnuplot.plot({'Segmentation Loss', iteration_seg, torch.Tensor(losses_seg), '-'})
gnuplot.xlabel('Iterations')
gnuplot.ylabel('Loss')
gnuplot.plotflush()
