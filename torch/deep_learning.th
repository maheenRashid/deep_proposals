require 'nn';
require 'loadcaffe';
require 'cutorch';
require 'cunn';
require 'image'
local clock = os.clock

function script_removeLayersFromCaffe(prototxt,model,out_file,bottom_layer_num)
	net = loadcaffe.load(prototxt,model);
	for i=net:size(),bottom_layer_num,-1 do
		net:remove(i)
	end

	-- print(net:size());
	-- for i=1,net:size() do
	-- 	print(i);
	-- 	print(net:get(i));
	-- end

	torch.save(out_file,net);
end

function makeMyNetwork(torch_file)
	vgg=torch.load(torch_file);


	score_branch=nn.Sequential();
	-- add interleaving here
	score_branch:add(nn.SpatialMaxPooling(2,2,2,2));
	score_branch:add(nn.View(512*7*7));
	score_branch:add(nn.Linear(512*7*7,512));
	score_branch:add(nn.ReLU());
	score_branch:add(nn.Dropout(0.5));
	score_branch:add(nn.Linear(512,1024));
	score_branch:add(nn.ReLU());
	score_branch:add(nn.Dropout(0.5));
	score_branch:add(nn.Linear(1024,1));
	-- print(score_branch);

	seg_branch=nn.Sequential();
	seg_branch:add(nn.SpatialConvolution(512,512,1,1));
	seg_branch:add(nn.View(512*14*14));
	seg_branch:add(nn.Linear(512*14*14,512));
	seg_branch:add(nn.Linear(512,56*56));
	seg_branch:add(nn.View(1,56,56));
	-- add bilinearupsampling here to get output of size 224*224 -done
	seg_branch:add(nn.SpatialUpSamplingNearest(4));
	-- print(seg_branch);

	split_net=nn.ConcatTable();
	split_net:add(score_branch);
	split_net:add(seg_branch);
	vgg:add(split_net);
	-- print(vgg)

	-- im=torch.rand(3,224,224);
	-- out=vgg:forward(im);
	-- print (vgg:get(31):get(2):get(1))
	return vgg
end

function makeSimpleNet()
	net = nn.Sequential()
	net:add(nn.SpatialConvolution(1, 6, 5, 5)) -- 1 input image channel, 6 output channels, 5x5 convolution kernel
	net:add(nn.Sigmoid())                       -- non-linearity 
	net:add(nn.SpatialMaxPooling(2,2,2,2))     -- A max-pooling operation that looks at 2x2 windows and finds the max.
	net:add(nn.SpatialConvolution(6, 16, 5, 5))
	net:add(nn.Sigmoid())                       -- non-linearity 
	net:add(nn.SpatialMaxPooling(2,2,2,2))
	net:add(nn.View(16*5*5))                    -- reshapes from a 3D tensor of 16x5x5 into 1D tensor of 16*5*5
	net:add(nn.Linear(16*5*5, 120))             -- fully connected layer (matrix multiplication between input and weights)
	net:add(nn.Sigmoid())                       -- non-linearity 
	net:add(nn.Linear(120, 84))
	net:add(nn.Sigmoid())                       -- non-linearity 
	net:add(nn.Linear(84, 10))                   -- 10 is the number of outputs of the network (in this case, 10 digits)
	net:add(nn.LogSoftMax()) 

	return net
end

function makeUpSamplingNetwork()
	net=nn.Sequential();
	net:add(nn.SpatialUpSamplingNearest(4));
	return net;
end

function testUpSamplingGPU()
	im = image.lena();
	im = image.scale(im,56,56);
	im_c=im:cuda();

	im_temp=im_c:double();
	im_big_org=image.scale(im_temp,224,224,'bilinear');
	im_big_org = im_big_org:typeAs(im_c);


	net=makeUpSamplingNetwork();
	net=net:cuda();
	im_big_net=net:forward(im_c);
	print(im_big_net:type());
	print(im_big_org:type());
	print(torch.all(torch.eq(im_big_org,im_big_net)))

	image.save('/disk2/temp/lena_net.jpg',im_big_net);
	image.save('/disk2/temp/lena_org.jpg',im_big_org);

end

function getLossSeg(x_seg,gt_mask)
	loss=torch.mul(gt_mask,-1)
	-- print(loss:size())
	-- print(gt_mask:size())
	loss:cmul(x_seg)
	loss:exp();
	loss:add(1);
	loss:log()
	loss=loss:sum()
	return loss;
end

function getLoss(x_score,x_seg,gt_label,gt_mask)
	if gt_label==-1 then
		loss_seg=0;
	else
		loss_seg=getLossSeg(x_seg,gt_mask);
		loss_seg= (1+gt_label[1])/(2*56*56)*loss_seg;
		print (loss_seg);
	end
	-- print('other');
	-- print(x_score)
	-- print(gt_label);
	loss_score=(1/32)*getLossSeg(x_score,gt_label);

	
	-- loss_score=0;
	print (loss_score)
	total_loss=	loss_seg+loss_score;
	return total_loss;
end

function getLossSegD(x_seg,gt_mask)
	dloss=torch.mul(x_seg,-1);
	dloss:sigmoid();
	dloss:mul(-1);
	dloss:cmul(gt_mask);
	dloss:div(56*56);
	return dloss;
end

function getLossScoreD(x_score,gt_label)
	-- print (x_score)
	dloss=torch.mul(x_score,-1);
	-- print(dloss);
	dloss:sigmoid();
	-- a=
	-- print(dloss);
	-- print (xscore);

	dloss:cmul(torch.mul(gt_label,-1.0));
	dloss:div(32);
	-- print (dloss);
	return dloss;
end


function sleep(n)  -- seconds
  local t0 = clock()
  while clock() - t0 <= n do end
end


prototxt = '/disk2/januaryExperiments/vgg_16/VGG_ILSVRC_16_layers_deploy.prototxt';
model = '/disk2/januaryExperiments/vgg_16/VGG_ILSVRC_16_layers.caffemodel';
torch_file='/disk2/januaryExperiments/vgg_16/vgg16_onlyConv.dat';
deep_prop_file='/disk2/februaryExperiments/deep_proposals/model_no_interleaving.dat';
batch_size=32;
learningRate=0.001;
momentum=0.9
weight_decay=0.00005;


bottom_layer_num=31;
cutorch.setDevice(1);
-- script_removeLayersFromCaffe(prototxt,model,torch_file,bottom_layer_num);
-- net = makeMyNetwork(torch_file);
-- torch.save(deep_prop_file,net);

net = torch.load(deep_prop_file);
net = net:cuda();
-- print (net);

-- trainer = nn.StochasticGradient(net, criterion)
-- trainer.learningRate = 0.001
-- trainer.maxIteration = 5 

-- x_seg=torch.Tensor({{1,2,3,4}, {5,6,7,8},{9,10,11,12},{13,14,15,16}})
-- gt_mask=torch.Tensor({{1,1,1,1},{1,0,0,1},{1,0,0,1},{1,1,1,1}});
gt_mask=torch.Tensor(1,224,224):zero();

gt_mask[1][10][10]=1;

gt_label=torch.Tensor({1})
-- torch.Tensor({1});
-- x_score=0.5
-- torch.Tensor({0.5});
print(gt_label)
-- print(x_score);

-- loss = getLoss(x_score,x_seg,gt_label,gt_mask);
-- print(loss);

im = image.lena();
im = image.scale(im,224,224,'bilinear');
out = net:forward(im:cuda());
-- out = net:forward(im);
print (out[2]:size())
print (out[1]:size())
print(out[1]);
print (out)

x_score=out[1];x_seg=out[2];
print('x_score '..tostring(x_score[1]));
print('x_seg '..tostring(x_seg[1][10][10]));

loss = getLoss(x_score:double(),x_seg:double(),gt_label,gt_mask);
print(loss);

loss_score_d=getLossScoreD(x_score:double(),gt_label)
print('loss_score_d = '..tostring(loss_score_d))
loss_seg_d=getLossSegD(x_seg:double(),gt_mask);
print('loss_seg_d:size() = '..tostring(loss_seg_d:size()))
print('loss_seg_d[1][10][10] = '..loss_seg_d[1][10][10]);

