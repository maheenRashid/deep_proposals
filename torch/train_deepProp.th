require 'nn';
require 'loadcaffe';
require 'cutorch';
require 'cunn';
require 'image'
npy4th = require 'npy4th'
require 'optim'

do
    local data = torch.class('data')

    function data:__init(args)
        -- print ('initing');
        self.file_path_positive='/disk2/februaryExperiments/deep_proposals/positive_data.txt';
        self.file_path_negative='/disk2/marchExperiments/deep_proposals/negatives.txt';

        self.batch_size_seg=32;
        self.batch_size_positive_score=16;
        self.batch_size_negative_score=16;


        self.start_idx_train=1;
        self.start_idx_positive_seg=1;
        self.start_idx_positive_score=1;
        self.start_idx_negative_score=1;

        self.params={jitter_size=16,
            crop_size={224,224},
            scale_range={285,224},
            mean={122,117,104},
            tolerance=48};
        self.training_set_seg={};
        self.training_set_score={};
        self.lines=self:readDataFile(self.file_path_positive)
        self.lines_negative=self:readDataFile(self.file_path_negative);
        
        print (#self.lines);
        print (#self.lines_negative);
        -- return self;
    end


    function data:getTrainingDataToyScore()
        local total_batch_size=self.batch_size_positive_score+self.batch_size_negative_score;

        self.training_set_score.data=torch.zeros(total_batch_size,3,self.params.crop_size[1],self.params.crop_size[2]);
        self.training_set_score.label=torch.zeros(total_batch_size,1);

        self.start_idx_positive_score=self:addTrainingDataPositive(self.training_set_score,
            self.batch_size_positive_score,
            self.lines,self.start_idx_positive_score,self.params,false)
        
        self.start_idx_negative_score=self:addTrainingDataNegativeScore(self.training_set_score,
            self.batch_size_negative_score,
            self.lines_negative,self.start_idx_negative_score,self.params,self.batch_size_positive_score+1)
    end

    function data:getTrainingDataToy()
        self.training_set_seg.data=torch.zeros(self.batch_size_seg,3,self.params.crop_size[1],self.params.crop_size[2]);
        self.training_set_seg.label=torch.zeros(self.batch_size_seg,1,self.params.crop_size[1],
        	self.params.crop_size[2]);

        self.start_idx_positive_seg=self:addTrainingDataPositive(self.training_set_seg,self.batch_size_seg,
        	self.lines,self.start_idx_positive_seg,self.params,true)
    end

    function data:readDataFile(file_path)
        -- print(file_path);
        local file_lines = {};
        for line in io.lines(file_path) do 
            -- print(file_path);
            local start_idx, end_idx = string.find(line, ' ');
            -- print(line);
            -- print (start_idx)
            local img_path=string.sub(line,1,start_idx-1);
            local img_label=string.sub(line,end_idx+1,#line);
            file_lines[#file_lines+1]={img_path,img_label};
            if #file_lines==100 then
                break;
            end
            -- print(file_path);
        end 
        -- print(#file_lines);
        return file_lines
    end

    function data:unique(input)
      local b = {}
      -- print (input:numel())
      local input=torch.reshape(input, input:numel())
      -- print (input:size());
      for i=1,input:numel() do
            -- print (i)
            -- print (input[i])
            if input[i]==-1 then
                b[100]=true;
            else
                b[input[i]] = true
            end
       end
      local out = {}
      for i in pairs(b) do
          table.insert(out,i)
       end
      return out
    end

    function data:jitterImage(img,mask,jitter_size,crop_size)
        local x=math.random(jitter_size-1);
        local y=math.random(jitter_size-1);
        -- print ('x y '..x..' '..y);
        local x_e=x+crop_size[1];
        local y_e=y+crop_size[2];
        -- print (img:size())
        -- print(x..' '..y..' '..x_e..' '..y_e)
        local img=image.crop(img,x,y,x_e,y_e);
        local mask=image.crop(mask,x,y,x_e,y_e);
        return img,mask
    end

    function data:scaleImage(img,mask,scale_range,crop_size)
        local new_scale=math.random(scale_range[1],scale_range[2]);
        -- print ('new_scale '..new_scale);
        local img=image.scale(img,new_scale);
        -- print (img:size())
        local mask=image.scale(mask,new_scale,'simple');
        -- print (mask:size())
        local start_x=math.floor((img:size()[2]-crop_size[1])/2);
        -- print (start_x)
        local start_y=math.floor((img:size()[3]-crop_size[2])/2);
        -- print (start_y);
        local img=image.crop(img,start_x,start_y,start_x+crop_size[1],start_y+crop_size[2]);
        -- print (img:size())
        local mask=image.crop(mask,start_x,start_y,start_x+crop_size[1],start_y+crop_size[2]);
        -- print (mask:size());
        return img,mask
    end

    function data:processImAndMaskPositive(img,mask,params)

        if img:size()[1]==1 then
            img= torch.cat(img,img,1):cat(img,1)
        end

        -- bring img to 0 255 range
        img:mul(255);

        -- bring mask to -1 +1 range
        mask:mul(255);
        mask:mul(2);
        mask:csub(1);

        -- jitter, scale or flip 

        local rand=math.random(3);
        if rand==1 then
            img,mask=self:jitterImage(img,mask,params.jitter_size,params.crop_size);
        elseif rand==2 then
            img,mask=self:scaleImage(img,mask,params.scale_range,params.crop_size);
        else
            img,mask=self:jitterImage(img,mask,params.jitter_size,params.crop_size);
            image.hflip(img,img);
            image.hflip(mask,mask);
        end

        -- subtract the mean
        for i=1,img:size()[1] do
            img[i]:csub(params.mean[i])
        end
        -- return
        return img,mask

    end


    function data:addTrainingDataPositive(training_set,num_im,list_files,start_idx,params,segFlag)
        local list_idx=start_idx;
        local list_size=#list_files;
        -- local count=0;
        -- print('list_idx_start '..list_idx);
        for curr_idx=1,num_im do
            -- count=count+1;
            -- add mod length here to make the batch size rotate
            -- print ('list_idx '..list_idx);
            local img_path=list_files[list_idx][1];
            local mask_path=list_files[list_idx][2];
            local img=image.load(img_path);
            if img:size()[1]==1 then
                img= torch.cat(img,img,1):cat(img,1)
            end
            local mask=image.load(mask_path);
            
            img,mask=self:processImAndMaskPositive(img,mask,params)
            training_set.data[curr_idx]=img;
            if segFlag then
                training_set.label[curr_idx]=mask;
            else
                training_set.label[curr_idx][1]=1;
            end
            list_idx=(list_idx%list_size)+1;
        end
        -- print( count);
        -- print('list_idx_end '..list_idx);
        return list_idx;
    end

    function data:isValidNegative(crop_coord,bbox,tolerance)
        local start_x=crop_coord[1];
        local start_y=crop_coord[2];
        local end_x=crop_coord[3];
        local end_y=crop_coord[4];
        local isValid=true;
        for box_no=1,bbox:size()[1] do
            local x_min_b=bbox[box_no][1];
            local y_min_b=bbox[box_no][2];
            local x_max_b=x_min_b+bbox[box_no][3];
            local y_max_b=y_min_b+bbox[box_no][4];
            local x_dim_b=bbox[box_no][3];
            local y_dim_b=bbox[box_no][4];
            local max_dim=(x_dim_b>y_dim_b) and x_dim_b or y_dim_b;
            -- is it entirely contained in the crop?
            if x_min_b>=start_x and y_min_b>=start_y and x_max_b<=end_x and y_max_b<=end_y then
                -- is it not too big or not too small
                if max_dim<128*2 and max_dim>128/2 then
                    -- is it offset enough?
                    if x_min_b-start_x<tolerance or y_min_b-start_y<tolerance then
                        isValid=false;
                        break;
                    end
                end
            end
        end
        return isValid;
    end

    function data:addTrainingDataNegativeScore(training_set,num_im,list_files,start_idx,params,training_data_idx)
        local list_idx=start_idx;
        local list_size=#list_files;
        -- print('list_idx_start '..list_idx);
        for curr_idx=training_data_idx,training_data_idx+num_im-1 do

            local img_path=list_files[list_idx][1];
            local npy_path=list_files[list_idx][2];
            local img=image.load(img_path);

            -- make sure image has 3 channels
            if img:size()[1]==1 then
                img= torch.cat(img,img,1):cat(img,1)
            end
            -- make sure image has range 255
            img:mul(255);
            -- subtract mean
            for i=1,img:size()[1] do
                img[i]:csub(params.mean[i])
            end
            -- load bbox and make in to int
            local bbox=npy4th.loadnpy(npy_path)
            bbox:floor();
            
            -- get max starting point of crop
            local x_max=img:size()[3]-params.crop_size[2];
            local y_max=img:size()[2]-params.crop_size[1];
            -- create a valid crop box that does not violate any positive examples
            local start_x;
            local start_y;
            local crop_box={};
            while 1 do
                start_x=torch.random(0,x_max-1);
                start_y=torch.random(0,y_max-1);
                local end_x=start_x+params.crop_size[2];
                local end_y=start_y+params.crop_size[1];
                crop_box={start_x,start_y,end_x,end_y};
                -- check if crop is valid negative
                if self:isValidNegative(crop_box,bbox,params.tolerance) then
                    break;
                end
            end
            -- add to training data
            training_set.data[curr_idx]=image.crop(img,crop_box[1],crop_box[2],crop_box[3],crop_box[4]);
            training_set.label[curr_idx][1]=-1;
            list_idx=(list_idx%list_size)+1;
        end
        -- print('list_idx_end '..list_idx);
        return list_idx;  
    end  

end

function getLossSeg(x_seg,gt_mask)
	local loss=torch.mul(gt_mask,-1)
	-- print(loss:size())
	-- print(gt_mask:size())
	loss:cmul(x_seg)
	loss:exp();
	loss:add(1);
	loss:log();
    loss:div(56*56);
	loss=loss:sum()
	return loss;
end

function getLossScore(x_score,gt_label)
    local loss=torch.mul(gt_label,-1)
    loss:cmul(x_score)
    loss:exp();
    loss:add(1);
    loss:log();
    loss:div(32);
    loss=loss:sum()
    return loss;
end

function getLossSegD(x_seg,gt_mask)
	local dloss=torch.mul(gt_mask,-1);
	dloss:cmul(x_seg);
    print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:exp();
    print (torch.min(dloss)..' '..torch.max(dloss));
	local deno=torch.add(dloss,1);
    print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:cmul(gt_mask);
    print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:mul(-1);
    print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:cdiv(deno);
    print (torch.min(dloss)..' '..torch.max(dloss));
	dloss:div(56*56);
    print (torch.min(dloss)..' '..torch.max(dloss));
	return dloss;
end

function getLossScoreD(x_score,gt_label)
	-- print (x_score)
	local dloss=torch.mul(gt_label,-1);
	dloss:cmul(x_score);
    dloss:exp();
    local deno=torch.add(dloss,1);
    dloss:cmul(gt_label);
    dloss:mul(-1);
    dloss:cdiv(deno);
    dloss:div(32);
    return dloss;
end

function makeMyNetworkJustScore(torch_file)
    local vgg=torch.load(torch_file);
    vgg:add(nn.SpatialMaxPooling(2,2,2,2));
    vgg:add(nn.View(512*7*7));
    vgg:add(nn.Linear(512*7*7,512));
    vgg:add(nn.ReLU());
    vgg:add(nn.Dropout(0.5));
    vgg:add(nn.Linear(512,1024));
    vgg:add(nn.ReLU());
    vgg:add(nn.Dropout(0.5));
    vgg:add(nn.Linear(1024,1));


    return vgg
end


prototxt = '/disk2/januaryExperiments/vgg_16/VGG_ILSVRC_16_layers_deploy.prototxt';
model = '/disk2/januaryExperiments/vgg_16/VGG_ILSVRC_16_layers.caffemodel';
torch_file='/disk2/januaryExperiments/vgg_16/vgg16_onlyConv.dat';
-- deep_prop_file='/disk2/februaryExperiments/deep_proposals/model_no_interleaving.dat';
-- deep_prop_file='/disk2/februaryExperiments/deep_proposals/model_no_seg.dat';
deep_prop_file='/disk2/februaryExperiments/deep_proposals/model_no_seg_trained_sgd.dat';
deep_prop_file='/disk2/februaryExperiments/deep_proposals/model_no_interleaving_chunks.dat'
out_file_pred='/disk2/februaryExperiments/deep_proposals/model_no_seg_test.npy';
out_file_gt='/disk2/februaryExperiments/deep_proposals/model_no_seg_gt.npy';
out_file_gt_data='/disk2/februaryExperiments/deep_proposals/model_no_seg_gt_data.npy';
out_dir_images='/disk2/marchExperiments/deep_proposals/checkPosNegData';

-- net=makeMyNetworkJustScore(torch_file)
-- print(net);
-- torch.save(out_file_net,net);


local opt = {}         
opt.optimization = 'sgd'

opt.batch_size=32;
opt.learningRate=0.00001;
opt.momentum=0.9
opt.weightDecay=0.00005;
opt.iterations=300;

local optimState       
local optimMethod      

optimState = {
    learningRate = opt.learningRate,
    weightDecay = opt.weightDecay,
    momentum = opt.momentum,
}

optimMethod = optim.sgd

print ('loading network');
-- net = torch.load(torch_file);

net = torch.load(deep_prop_file);
-- net = net:cuda();
print ('done loading network');
-- print (net:get(1));

print (net);
-- print (net:get(31));
-- print (net:get(31));
-- print (torch.sum(net:get(29).weight))

-- print (net:get(31):get(1):backward())

td = data();

local parameters, gradParameters = net:getParameters()
local counter = 0

local fevalScore = function(x)
    if x ~= parameters then
    parameters:copy(x)
    end
    
    td:getTrainingDataToyScore();    
    local batch_inputs = td.training_set_score.data:cuda();
    local batch_targets = td.training_set_score.label:cuda();
    
    gradParameters:zero()
    -- In order, these lines compute:
    -- 1. compute outputs (log probabilities) for each data point
    
    local batch_outputs_mid = net:get(1):forward(batch_inputs)
    local batch_outputs = net:get(2):get(1):forward(batch_outputs_mid)
    -- print (batch_outputs:size())

    -- batch_outputs=batch_outputs[1];
    -- print (batch_outputs);
    -- 2. compute the loss of these outputs, measured against the true labels in batch_target
    local batch_loss = getLossScore(batch_outputs, batch_targets)
    -- 3. compute the derivative of the loss wrt the outputs of the model
    local dloss_doutput = getLossScoreD(batch_outputs, batch_targets)
    
    print ('loss '..batch_loss..' dloss '..torch.sum(dloss_doutput))

    -- 4. use gradients to update weights, we'll understand this step more next week
    -- get(31):get(1):
    local gradInputs=net:get(2):get(1):backward(batch_inputs, dloss_doutput)
    -- print (gradInputs:size())
    net:get(1):backward(batch_inputs, gradInputs)
    -- print (net:get(5))
    print (torch.sum(net:get(2):get(2):get(1).weight))
    print (torch.sum(net:get(1):get(29).weight))
    -- print (torch.sum(net:get(1):get(29).weight))
    -- optim expects us to return
    --     loss, (gradient of loss with respect to the weights that we're optimizing)
    return batch_loss, gradParameters
end


local fevalSeg = function(x)
    if x ~= parameters then
    parameters:copy(x)
    end
    
    td:getTrainingDataToy();    
    local batch_inputs = td.training_set_seg.data:clone()
    -- :cuda();
    local batch_targets = td.training_set_seg.label:clone();
    -- :cuda();
    print (batch_inputs:size())
    print (batch_targets:size())
    gradParameters:zero()
    -- In order, these lines compute:
    -- 1. compute outputs (log probabilities) for each data point
    
    local batch_outputs_mid = net:get(1):forward(batch_inputs)
    -- print (batch_outputs_mid:size())
    local batch_outputs = net:get(2):get(2):forward(batch_outputs_mid)
    -- print (batch_outputs:size())

    -- batch_outputs=batch_outputs[1];
    -- print (batch_outputs);
    -- 2. compute the loss of these outputs, measured against the true labels in batch_target
    local batch_loss = getLossSeg(batch_outputs, batch_targets)
    -- 3. compute the derivative of the loss wrt the outputs of the model
    local dloss_doutput = getLossSegD(batch_outputs, batch_targets)
    
    print ('loss '..batch_loss..' dloss '..torch.sum(dloss_doutput))

    -- 4. use gradients to update weights, we'll understand this step more next week
    -- get(31):get(1):
    local gradInputs=net:get(2):get(2):backward(batch_inputs, dloss_doutput)
    -- print (gradInputs:size())
    net:get(1):backward(batch_inputs, gradInputs)
    -- print (net:get(5))
    print (torch.sum(net:get(2):get(1):get(3).weight))
    print (torch.sum(net:get(1):get(29).weight))
    -- optim expects us to return
    --     loss, (gradient of loss with respect to the weights that we're optimizing)
    return batch_loss, gradParameters
end


local losses = {}  

for i=1,opt.iterations do
    print ('iter,'..i);
    if i%2==0 then
        local _, minibatch_loss = optimMethod(fevalSeg,parameters, optimState)
        print(string.format("minibatches processed: %6s, loss = %6.6f", i, minibatch_loss[1]))
        losses[#losses + 1] = minibatch_loss[1] -- append the new loss
    else
        -- local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
        -- print(string.format("minibatches processed: %6s, loss = %6.6f", i, minibatch_loss[1]))
        -- losses[#losses + 1] = minibatch_loss[1] -- append the new loss
    end
    -- print (optimState)
    -- print (minibatch_loss.shape)
    -- if i % 10 == 0 then -- don't print *every* iteration, this is enough to get the gist
    

end

-- torch.save(out_file_net,net);
