require 'nn';
require 'loadcaffe';
require 'cutorch';
require 'cunn';
require 'optim'
require 'hdf5'
require 'image'
require 'gnuplot'
npy4th = require 'npy4th'
-- require 'data_noFlow';
require 'paths';
-- require 'cc2'


deploy_file='./opt_train_coarse_xavier.prototxt';
model_file='/home/maheenrashid/Downloads/debugging_jacob/optical_flow_prediction/examples/opticalflow/final.caffemodel';

net = loadcaffe.load(deploy_file,model_file,'cunn');
print (net)


-- -- local pos_file='/disk2/februaryExperiments/deep_proposals/positive_data.txt';
-- local pos_file='/disk2/aprilExperiments/positives_160.txt';
-- local neg_file='/disk2/marchExperiments/deep_proposals/negatives.txt';

-- paths.mkdir(out_dir);
-- local out_dir_intermediate=paths.concat(out_dir,'intermediate_resume_2');
-- local out_dir_final=paths.concat(out_dir,'final');
-- paths.mkdir(out_dir_intermediate);
-- paths.mkdir(out_dir_final);

-- local out_file_net=out_dir_final..'/'..'model_all_final.dat';

-- local out_file_loss_seg=out_dir_final..'/'..'loss_all_final_seg.npy';
-- local out_file_loss_score=out_dir_final..'/'..'loss_all_final_score.npy';

-- local out_file_intermediate_pre = out_dir_intermediate..'/'..'model_all_';
-- local out_file_loss_intermediate_pre = out_dir_intermediate..'/'..'loss_all_';

-- local out_file_loss_plot_intermediate_score=out_dir_intermediate..'/'..'loss_iter_score.png';
-- local out_file_loss_plot_final_score=out_dir_final..'/'..'loss_iter_score.png';

-- local out_file_loss_plot_intermediate_seg=out_dir_intermediate..'/'..'loss_iter_seg.png';
-- local out_file_loss_plot_final_seg=out_dir_final..'/'..'loss_iter_seg.png';


-- -- print ('pos_file',pos_file)
-- -- print ('neg_file',neg_file)
-- -- td=data({file_path_positive=pos_file,file_path_negative=neg_file});
-- -- print (td.lines_positive[1])
-- -- print (td.lines_negative[1])
-- -- -- td:getTrainingDataToy();
-- -- td:getTrainingDataToyScore();
-- -- print (td.training_set_score.data:size())
-- -- for i=1,td.training_set_score.data:size(1) do
-- --     print (i,torch.min(td.training_set_score.data[i]),torch.max(td.training_set_score.data[i]),td.training_set_score.label[i][1])
-- -- end



-- local opt = {}         
-- opt.optimization = 'sgd'
-- opt.batch_size=32;
-- opt.learningRate=0.001;
-- opt.momentum=0.9
-- opt.weightDecay=0.00005;
-- opt.iterations=100000;
-- opt.saveAfter=5000;
-- opt.plotAfter=50;
-- opt.dispAfter=4;

-- cutorch.setDevice(1);

-- local optimState       
-- local optimMethod      

-- optimState = {
--     learningRate = opt.learningRate,
--     weightDecay = opt.weightDecay,
--     momentum = opt.momentum,
-- }

-- optimMethod = optim.sgd


-- print ('loading network');

-- net = torch.load(deep_prop_file);
-- -- net:get(2):get(2):add(nn.Tanh())
-- print (torch.min(net:get(1):get(2):get(3).weight),torch.min(net:get(1):get(2):get(3).weight));

-- print ('done loading network');
-- -- print (net:get(1));
-- print (net);

-- print ('making cuda');
-- net = net:cuda();
-- print ('done');

-- print ('loading params');
-- local parameters, gradParameters = net:getParameters()
-- print ('loading done');
-- print (optimState)
-- print ('pos_file',pos_file)
-- print ('neg_file',neg_file)
-- td=data({file_path_positive=pos_file,file_path_negative=neg_file});

-- local counter = 0

-- local fevalScore = function(x)
--     if x ~= parameters then
--     parameters:copy(x)
--     end
    
--     td:getTrainingDataToyScore();    
--     local batch_inputs = td.training_set_score.data:cuda();
--     -- local batch_inputs_flow = td.training_set_score.data_flow:cuda();
--     local batch_targets = td.training_set_score.label:cuda();
    
--     gradParameters:zero()
    
--     local batch_outputs_mid = net:get(1):forward(batch_inputs)
--     local batch_outputs = net:get(2):get(1):forward(batch_outputs_mid)

--     local batch_loss = getLossScore(batch_outputs, batch_targets)
--     local dloss_doutput = getLossScoreD(batch_outputs, batch_targets)
--     local gradInputs=net:get(2):get(1):backward(batch_outputs_mid, dloss_doutput)
--     net:get(1):backward(batch_inputs, gradInputs)
--     return batch_loss, gradParameters
-- end


-- local fevalSeg = function(x)
--     if x ~= parameters then
--     parameters:copy(x)
--     end
    
--     td:getTrainingDataToy();    
--     local batch_inputs = td.training_set_seg.data:cuda();
--     -- local batch_inputs_flow = td.training_set_seg.data_flow:cuda();
--     local batch_targets = td.training_set_seg.label:cuda();

--     gradParameters:zero()
--     local batch_outputs_mid = net:get(1):forward(batch_inputs)
--     local batch_outputs = net:get(2):get(2):forward(batch_outputs_mid)
--     local batch_loss = getLossSeg(batch_outputs, batch_targets)
--     local dloss_doutput = getLossSegD(batch_outputs, batch_targets)
    
--     local gradInputs=net:get(2):get(2):backward(batch_outputs_mid, dloss_doutput):clone()
--     net:get(1):backward(batch_inputs, gradInputs)
--     return batch_loss, gradParameters
-- end


-- local losses_seg = {};
-- local losses_score = {};

-- for i=1,opt.iterations do
    
--     if i%2==0 then
--         local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
--         losses_score[#losses_score + 1] = minibatch_loss[1] -- append the new loss        
--     else
--         local _, minibatch_loss = optimMethod(fevalSeg,parameters, optimState)
--         losses_seg[#losses_seg + 1] = minibatch_loss[1] -- append the new loss

--     end

--     if i%opt.dispAfter==0 then
--         print(string.format("minibatches processed: %6s, loss seg = %6.6f, loss score = %6.6f", i, 
--             losses_seg[#losses_seg],losses_score[#losses_score]))
--     end

--     -- check if model needs to be saved. save it.
--     -- also save losses
--     if i%opt.saveAfter==0 then
--         local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
--         torch.save(out_file_intermediate,net);
--         local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_score.npy';
--         npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses_score))
--         local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_seg.npy';
--         npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses_seg))
--     end
    
--     -- check if losses need to be plotted. plot them.
--     -- if i%opt.plotAfter==0 then
        
--     --     gnuplot.pngfigure(out_file_loss_plot_intermediate_score)
--     --     local iteration_score = torch.range( 1, #losses_score)
--     --     gnuplot.plot({'Score Loss', iteration_score, torch.Tensor(losses_score),  '-'})
--     --     gnuplot.xlabel('Iterations')
--     --     gnuplot.ylabel('Loss')
--     --     gnuplot.plotflush()

--     --     gnuplot.pngfigure(out_file_loss_plot_intermediate_seg)
--     --     local iteration_seg = torch.range( 1, #losses_seg)
--     --     gnuplot.plot({'Segmentation Loss', iteration_seg, torch.Tensor(losses_seg), '-'})
--     --     gnuplot.xlabel('Iterations')
--     --     gnuplot.ylabel('Loss')
--     --     gnuplot.plotflush()
--     -- end

-- end

-- -- save final model
-- torch.save(out_file_net,net);
-- npy4th.savenpy(out_file_loss_seg, torch.Tensor(losses_seg))
-- npy4th.savenpy(out_file_loss_score, torch.Tensor(losses_score))

-- -- save final plot

-- -- gnuplot.pngfigure(out_file_loss_plot_final_score)
-- -- local iteration_score = torch.range( 1, #losses_score)
-- -- gnuplot.plot({'Score Loss', iteration_score, torch.Tensor(losses_score),  '-'})
-- -- gnuplot.xlabel('Iterations')
-- -- gnuplot.ylabel('Loss')
-- -- gnuplot.plotflush()

-- -- gnuplot.pngfigure(out_file_loss_plot_final_seg)
-- -- local iteration_seg = torch.range( 1, #losses_seg)
-- -- gnuplot.plot({'Segmentation Loss', iteration_seg, torch.Tensor(losses_seg), '-'})
-- -- gnuplot.xlabel('Iterations')
-- -- gnuplot.ylabel('Loss')
-- -- gnuplot.plotflush()
