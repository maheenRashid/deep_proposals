require 'nn';
require 'loadcaffe';
require 'cutorch';
require 'cunn';
require 'optim'
require 'hdf5'
require 'image'
require 'gnuplot'
npy4th = require 'npy4th'
require 'data_noFlow';
require 'paths';
w_init=require('weight-init');


function makeXavierGaussian(model)
    for idx=1,#model do
        
        local m = model.modules[idx]
        if m.weight then
            local var;
            if m.__typename == 'nn.SpatialConvolution' then
                var = {m.nInputPlane*m.kH*m.kW, m.nOutputPlane*m.kH*m.kW}
            elseif m.__typename == 'nn.SpatialConvolutionMM' then
                var = {m.nInputPlane*m.kH*m.kW, m.nOutputPlane*m.kH*m.kW}
            elseif m.__typename == 'nn.LateralConvolution' then
                var = {m.nInputPlane*1*1, m.nOutputPlane*1*1}
            elseif m.__typename == 'nn.VerticalConvolution' then
                var = {1*m.kH*m.kW, 1*m.kH*m.kW}
            elseif m.__typename == 'nn.HorizontalConvolution' then
                var = {1*m.kH*m.kW, 1*m.kH*m.kW}
            elseif m.__typename == 'nn.Linear' then
                var = {m.weight:size(2), m.weight:size(1)}
            elseif m.__typename == 'nn.TemporalConvolution' then
                var = {m.weight:size(2), m.weight:size(1)}
            end

            var = 2/(var[1] + var[2])
            m.weight=torch.randn(m.weight:size()):mul(torch.sqrt(var));
            m.bias=torch.zeros(m.bias:size());
            
        end
    end

    return model
end

function makeMyNetwork(torch_file)
    method_init='xavier';
	local vgg=torch.load(torch_file);

	local seq=nn.Sequential();
	seq:add(vgg);

	local seq_trunk = nn.Sequential();
    seq_trunk:add(nn.SpatialConvolution(512,128,1,1));
    seq_trunk:add(nn.View(128*10*10));
    seq_trunk:add(nn.Linear(128*10*10,512));

    -- for idx=1,#seq_trunk do
    --     if seq_trunk:get(idx).weight then
    --         seq_trunk:get(idx).weight=torch.randn(seq_trunk:get(idx).weight:size()):div(100);
    --     end
    --     if seq_trunk:get(idx).bias then
    --         seq_trunk:get(idx).bias=torch.zeros(seq_trunk:get(idx).bias:size());
    --     end
    -- end
    -- seq_trunk=w_init(seq_trunk,method_init);
    seq_trunk = makeXavierGaussian(seq_trunk)
	-- print (torch.min(seq_trunk:get(1).weight),torch.max(seq_trunk:get(1).weight))
    seq:add(seq_trunk);

	local score_branch=nn.Sequential();
	score_branch:add(nn.Linear(512,1024));
	score_branch:add(nn.ReLU());
	score_branch:add(nn.Dropout(0.5));
	score_branch:add(nn.Linear(1024,1));
	-- score_branch:add(nn.Sigmoid());

    -- for idx=1,#score_branch do
    --     if score_branch:get(idx).weight then
    --         score_branch:get(idx).weight=torch.randn(score_branch:get(idx).weight:size()):div(100);
    --     end
    --     if score_branch:get(idx).bias then
    --         score_branch:get(idx).bias=torch.zeros(score_branch:get(idx).bias:size());
    --     end
    -- end
    -- score_branch=w_init(score_branch,method_init);
    score_branch = makeXavierGaussian(score_branch)

	local seg_branch=nn.Sequential();
	seg_branch:add(nn.Linear(512,40*40));
	seg_branch:add(nn.View(1,40,40));
	seg_branch:add(nn.SpatialUpSamplingNearest(4));
	-- seg_branch:add(nn.Sigmoid());

    -- for idx=1,#seg_branch do
    --     if seg_branch:get(idx).weight then
    --         seg_branch:get(idx).weight=torch.randn(seg_branch:get(idx).weight:size()):div(100);
    --     end
    --     if seg_branch:get(idx).bias then
    --         seg_branch:get(idx).bias=torch.zeros(seg_branch:get(idx).bias:size());
    --     end
    -- end
    -- seg_branch=w_init(seg_branch,method_init);
    seg_branch = makeXavierGaussian(seg_branch)

	local split_net=nn.ConcatTable();
	split_net:add(score_branch);
	split_net:add(seg_branch);

    local total_net = nn.Sequential();
    total_net:add(seq);
    total_net:add(split_net);

	return total_net
end

function getLossSeg(x_seg,gt_mask)
	local loss=torch.mul(gt_mask,-1)
	-- print(loss:size())
	-- print(gt_mask:size())
	loss:cmul(x_seg)
	loss:exp();
	loss:add(1);
	loss:log();
    loss:div(40*40);
	loss=loss:sum()
	return loss;
end

function getLossScore(x_score,gt_label)
    local loss=torch.mul(gt_label,-1)
    loss:cmul(x_score)
    loss:exp();
    loss:add(1);
    loss:log();
    loss:div(32);
    loss=loss:sum()
    return loss;
end

function getLossSegD(x_seg,gt_mask)
	local dloss=torch.mul(gt_mask,-1);
	dloss:cmul(x_seg);
	dloss:exp();
	local deno=torch.add(dloss,1);
	dloss:cmul(gt_mask);
	dloss:mul(-1);
	dloss:cdiv(deno);
	dloss:div(40*40);
	return dloss;
end

function getLossScoreD(x_score,gt_label)
	-- print (x_score)
	local dloss=torch.mul(gt_label,-1);
	dloss:cmul(x_score);
    dloss:exp();
    local deno=torch.add(dloss,1);
    dloss:cmul(gt_label);
    dloss:mul(-1);
    dloss:cdiv(deno);
    dloss:div(32);
    return dloss;
end

local torch_file='/disk2/januaryExperiments/vgg_16/vgg16_onlyConv.dat';
local deep_prop_file='/disk2/aprilExperiments/headC_160/noFlow_gaussian_softmax.dat'
-- local deep_prop_file = '/disk3/maheen_data/headC_160/noFlow_gaussian_human_softmax/intermediate/model_all_5500.dat';
-- local deep_prop_file='/disk2/aprilExperiments/headC_160/noFlow_gaussian_all_actual/intermediate_resume/model_all_25000.dat';
-- local out_dir='/disk2/aprilExperiments/headC_160/noFlow_gaussian_all_actual';
local out_dir='/disk3/maheen_data/headC_160/noFlow_gaussian_all_softmax_temp';


local torch_file='/disk2/januaryExperiments/vgg_16/vgg16_onlyConv.dat';
local deep_prop_file='/disk2/aprilExperiments/headC_160/noFlow_gaussian_xavier_fixed.dat'
-- local deep_prop_file='/disk3/maheen_data/headC_160/withFlow_gaussian_human_softmax/final/model_all_final.dat'
local out_dir='/disk3/maheen_data/headC_160/noFlow_testSubset_32';


net=makeMyNetwork(torch_file)
torch.save(deep_prop_file,net);


-- local pos_file='/disk2/februaryExperiments/deep_proposals/positive_data.txt';
-- local pos_file='/disk2/aprilExperiments/positives_160.txt';
-- local neg_file='/disk2/marchExperiments/deep_proposals/negatives.txt';

-- local val_pos_file='/disk2/aprilExperiments/positives_160.txt';
-- local val_neg_file='/disk2/marchExperiments/deep_proposals/negatives.txt';


local pos_file='/disk2/aprilExperiments/positives_160_oneHundreth.txt';
local neg_file='/disk2/marchExperiments/deep_proposals/negatives_oneHundreth.txt';

local val_pos_file='/disk2/aprilExperiments/positives_160_oneHundreth.txt';
local val_neg_file='/disk2/marchExperiments/deep_proposals/negatives_oneHundreth.txt';

paths.mkdir(out_dir);
local out_dir_intermediate=paths.concat(out_dir,'intermediate');
local out_dir_final=paths.concat(out_dir,'final');
local out_dir_debug = paths.concat(out_dir,'debug');
paths.mkdir(out_dir_intermediate);
paths.mkdir(out_dir_final);
paths.mkdir(out_dir_debug);

local out_file_debug_pre = out_dir_debug..'/'..'debug_files_';
local out_file_net=out_dir_final..'/'..'model_all_final.dat';

local out_file_loss_seg=out_dir_final..'/'..'loss_all_final_seg.npy';
local out_file_loss_score=out_dir_final..'/'..'loss_all_final_score.npy';

local out_file_intermediate_pre = out_dir_intermediate..'/'..'model_all_';
local out_file_loss_intermediate_pre = out_dir_intermediate..'/'..'loss_all_';

local out_file_loss_plot_intermediate_score=out_dir_intermediate..'/'..'loss_iter_score.png';
local out_file_loss_plot_final_score=out_dir_final..'/'..'loss_iter_score.png';

local out_file_loss_plot_intermediate_seg=out_dir_intermediate..'/'..'loss_iter_seg.png';
local out_file_loss_plot_final_seg=out_dir_final..'/'..'loss_iter_seg.png';


print ('pos_file',pos_file)
print ('neg_file',neg_file)
-- td=data({file_path_positive=pos_file,file_path_negative=neg_file});
-- print (td.lines_positive[1])
-- print (td.lines_negative[1])
-- td:getTrainingDataToy();
-- print (td.training_set_seg.data:size())
-- for i=1,td.training_set_seg.data:size(1) do
--     print (i,torch.min(td.training_set_seg.data[i]),torch.max(td.training_set_seg.data[i]),td.training_set_seg.label[i]:size())
-- end

-- td:getTrainingDataToyScore();
-- print (td.training_set_score.data:size())
-- for i=1,td.training_set_score.data:size(1) do
--     print (i,torch.min(td.training_set_score.data[i]),torch.max(td.training_set_score.data[i]),td.training_set_score.label[i][1])
-- end


local opt = {}         
opt.optimization = 'sgd'
opt.batch_size=32;
opt.learningRate=0.0001;
opt.testAfter=0;
opt.momentum=0.9
opt.weightDecay=0.00005;
opt.iterations=200000;
opt.saveAfter=5000;
opt.plotAfter=50;
opt.dispAfter=4;

opt.debug=false;
-- opt.debug = 1;

cutorch.setDevice(1);

local optimState       
local optimMethod      

optimState = {
    learningRate = opt.learningRate,
    weightDecay = opt.weightDecay,
    momentum = opt.momentum,
}

optimMethod = optim.sgd


print ('loading network');

net = torch.load(deep_prop_file);
-- net:get(2):get(2):add(nn.Tanh())
print (torch.min(net:get(1):get(2):get(3).weight),torch.min(net:get(1):get(2):get(3).weight));

print ('done loading network');
-- print (net:get(1));
print (net);

print ('making cuda');
net = net:cuda();
print ('done');

print ('loading params');
local parameters, gradParameters = net:getParameters()
print ('loading done');
print (optimState)
print ('pos_file',pos_file)
print ('neg_file',neg_file)
td=data({file_path_positive=pos_file,file_path_negative=neg_file});
vd=data({file_path_positive=val_pos_file,file_path_negative=val_neg_file});

local counter = 0

local fevalScore = function(x)
    if x ~= parameters then
    parameters:copy(x)
    end
    
    td:getTrainingDataToyScore();    
    
    local batch_inputs = td.training_set_score.data:cuda();
    -- local batch_inputs_flow = td.training_set_score.data_flow:cuda();
    local batch_targets = td.training_set_score.label:cuda();
    
    if debug_curr then
        print (out_file_debug_input_data,out_file_debug_input_label)
        npy4th.savenpy(out_file_debug_input_data, batch_inputs:double())
        npy4th.savenpy(out_file_debug_input_label, batch_targets:double())
    end
    
    gradParameters:zero()
    
    local batch_outputs_mid = net:get(1):forward(batch_inputs)
    local batch_outputs = net:get(2):get(1):forward(batch_outputs_mid)

    if debug_curr then
        print (out_file_debug_output)
        npy4th.savenpy(out_file_debug_output, batch_outputs:double())
    end
    

    local batch_loss = getLossScore(batch_outputs, batch_targets)

    if debug_curr then
        print (out_file_debug_loss)
        npy4th.savenpy(out_file_debug_loss, torch.Tensor({batch_loss}))
    end
        
    local dloss_doutput = getLossScoreD(batch_outputs, batch_targets)

    if debug_curr then
        print (out_file_debug_dloss)
        npy4th.savenpy(out_file_debug_dloss, dloss_doutput:double())
    end
    
    local gradInputs=net:get(2):get(1):backward(batch_outputs_mid, dloss_doutput):clone()
    net:get(1):backward(batch_inputs, gradInputs)
    return batch_loss, gradParameters
end


local fevalSeg = function(x)
    if x ~= parameters then
    parameters:copy(x)
    end
    
    td:getTrainingDataToy();

    local batch_inputs = td.training_set_seg.data:cuda();
    -- local batch_inputs_flow = td.training_set_seg.data_flow:cuda();
    local batch_targets = td.training_set_seg.label:cuda();

    if debug_curr then
        print (out_file_debug_input_data,out_file_debug_input_label)
        npy4th.savenpy(out_file_debug_input_data, batch_inputs:double())
        npy4th.savenpy(out_file_debug_input_label, batch_targets:double())
    end
    

    gradParameters:zero()
    local batch_outputs_mid = net:get(1):forward(batch_inputs)
    local batch_outputs = net:get(2):get(2):forward(batch_outputs_mid)

    if debug_curr then
        print (out_file_debug_output)
        npy4th.savenpy(out_file_debug_output, batch_outputs:double())
    end
    
    local batch_loss = getLossSeg(batch_outputs, batch_targets)

    if debug_curr then
        print (out_file_debug_loss)
        npy4th.savenpy(out_file_debug_loss, torch.Tensor({batch_loss}))
    end
    
    local dloss_doutput = getLossSegD(batch_outputs, batch_targets)
    
    if debug_curr then
        print (out_file_debug_dloss)
        npy4th.savenpy(out_file_debug_dloss, dloss_doutput:double())
    end
    
    local gradInputs=net:get(2):get(2):backward(batch_outputs_mid, dloss_doutput):clone()
    -- print (gradInputs:size());
    net:get(1):backward(batch_inputs, gradInputs)
    return batch_loss, gradParameters
end


local losses_seg = {};
local losses_score = {};
local val_losses_seg = {};
local val_losses_score = {};

for i=1,opt.iterations do
    
    if (i%opt.dispAfter==0 or i%opt.dispAfter==opt.dispAfter-1) and opt.debug then
        debug_curr = true;
    else
        debug_curr = false;
    end
    -- print (debug_curr,opt.debug)
    if opt.debug then
        if i%2==0 then
            out_file_debug_pre_curr=out_file_debug_pre..'score_';
        else 
            out_file_debug_pre_curr=out_file_debug_pre..'seg_';
        end
        out_file_debug_input_data=out_file_debug_pre_curr..'inputData_'..i..'.npy';
        out_file_debug_input_label=out_file_debug_pre_curr..'inputLabel_'..i..'.npy';
        out_file_debug_output=out_file_debug_pre_curr..'output_'..i..'.npy';
        out_file_debug_dloss=out_file_debug_pre_curr..'dloss_'..i..'.npy';
        out_file_debug_loss=out_file_debug_pre_curr..'loss_'..i..'.npy';
    end

    if i%2==0 then
        local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
        losses_score[#losses_score + 1] = minibatch_loss[1] -- append the new loss        
    else
        local _, minibatch_loss = optimMethod(fevalSeg,parameters, optimState)
        losses_seg[#losses_seg + 1] = minibatch_loss[1] -- append the new loss

    end

    if i%opt.dispAfter==0 then
        print(string.format("minibatches processed: %6s, loss seg = %6.6f, loss score = %6.6f", i, 
            losses_seg[#losses_seg],losses_score[#losses_score]))
    end


    if i%opt.testAfter==0 and opt.testAfter>0 then 

        net:evaluate();
        
        vd:getTrainingDataToyScore();
        local batch_inputs = vd.training_set_score.data:cuda();
        local batch_targets = vd.training_set_score.label:cuda();

        local out_results=net:forward(batch_inputs)
        local score_loss_val = getLossScore(out_results[1], batch_targets)

        vd:getTrainingDataToy();
        
        batch_inputs = vd.training_set_seg.data:cuda();
        batch_targets = vd.training_set_seg.label:cuda();
        
        out_results=net:forward(batch_inputs)
        local seg_loss_val = getLossSeg(out_results[2],batch_targets);
        
        val_losses_seg[#val_losses_seg+1]=seg_loss_val;
        val_losses_score[#val_losses_score+1]=score_loss_val;

        net:training();

        print(string.format("minibatches processed: %6s, val loss seg = %6.6f, val loss score = %6.6f", i, 
            val_losses_seg[#val_losses_seg],val_losses_score[#val_losses_score]))
    end


    -- check if model needs to be saved. save it.
    -- also save losses
    if i%opt.saveAfter==0 then
        local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
        torch.save(out_file_intermediate,net);
        local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_score.npy';
        npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses_score))
        local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_seg.npy';
        npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses_seg))

        if opt.testAfter>0 then 
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val_score.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses_score))
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val_seg.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses_seg))
        end
    end
    
    -- check if losses need to be plotted. plot them.
    -- if i%opt.plotAfter==0 then
        
    --     gnuplot.pngfigure(out_file_loss_plot_intermediate_score)
    --     local iteration_score = torch.range( 1, #losses_score)
    --     gnuplot.plot({'Score Loss', iteration_score, torch.Tensor(losses_score),  '-'})
    --     gnuplot.xlabel('Iterations')
    --     gnuplot.ylabel('Loss')
    --     gnuplot.plotflush()

    --     gnuplot.pngfigure(out_file_loss_plot_intermediate_seg)
    --     local iteration_seg = torch.range( 1, #losses_seg)
    --     gnuplot.plot({'Segmentation Loss', iteration_seg, torch.Tensor(losses_seg), '-'})
    --     gnuplot.xlabel('Iterations')
    --     gnuplot.ylabel('Loss')
    --     gnuplot.plotflush()
    -- end

end

-- save final model
torch.save(out_file_net,net);
npy4th.savenpy(out_file_loss_seg, torch.Tensor(losses_seg))
npy4th.savenpy(out_file_loss_score, torch.Tensor(losses_score))

-- save final plot

-- gnuplot.pngfigure(out_file_loss_plot_final_score)
-- local iteration_score = torch.range( 1, #losses_score)
-- gnuplot.plot({'Score Loss', iteration_score, torch.Tensor(losses_score),  '-'})
-- gnuplot.xlabel('Iterations')
-- gnuplot.ylabel('Loss')
-- gnuplot.plotflush()

-- gnuplot.pngfigure(out_file_loss_plot_final_seg)
-- local iteration_seg = torch.range( 1, #losses_seg)
-- gnuplot.plot({'Segmentation Loss', iteration_seg, torch.Tensor(losses_seg), '-'})
-- gnuplot.xlabel('Iterations')
-- gnuplot.ylabel('Loss')
-- gnuplot.plotflush()
