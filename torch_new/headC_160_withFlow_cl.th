require 'nn';
require 'loadcaffe';
require 'cutorch';
require 'cunn';
require 'optim'
require 'hdf5'
require 'image'
require 'gnuplot'
npy4th = require 'npy4th'
require 'data_withFlow';
require 'paths';


function makeXavierGaussian(model)
    for idx=1,#model do
        
        local m = model.modules[idx]
        if m.weight then
            local var;
            if m.__typename == 'nn.SpatialConvolution' then
                var = {m.nInputPlane*m.kH*m.kW, m.nOutputPlane*m.kH*m.kW}
            elseif m.__typename == 'nn.SpatialConvolutionMM' then
                var = {m.nInputPlane*m.kH*m.kW, m.nOutputPlane*m.kH*m.kW}
            elseif m.__typename == 'nn.LateralConvolution' then
                var = {m.nInputPlane*1*1, m.nOutputPlane*1*1}
            elseif m.__typename == 'nn.VerticalConvolution' then
                var = {1*m.kH*m.kW, 1*m.kH*m.kW}
            elseif m.__typename == 'nn.HorizontalConvolution' then
                var = {1*m.kH*m.kW, 1*m.kH*m.kW}
            elseif m.__typename == 'nn.Linear' then
                var = {m.weight:size(2), m.weight:size(1)}
            elseif m.__typename == 'nn.TemporalConvolution' then
                var = {m.weight:size(2), m.weight:size(1)}
            end

            var = 2/(var[1] + var[2])
            m.weight=torch.randn(m.weight:size()):mul(torch.sqrt(var));
            m.bias=torch.zeros(m.bias:size());
            print (var,torch.var(m.weight));
        end
    end

    return model
end

function makeMyNetwork(torch_file)


    local vgg=torch.load(torch_file);
    vgg=vgg:double();

    local para=nn.ParallelTable();
    para:add(vgg);
    para:add(vgg:clone());
    
    local seq=nn.Sequential();
    seq:add(para);

    local seq_trunk = nn.Sequential();
    seq_trunk:add(nn.JoinTable(2));
    seq_trunk:add(nn.SpatialConvolution(1024,512,1,1));
    seq_trunk:add(nn.SpatialConvolution(512,128,1,1));
    seq_trunk:add(nn.View(128*10*10));
    seq_trunk:add(nn.Linear(128*10*10,512));

    seq_trunk = makeXavierGaussian(seq_trunk)

    -- print (seq_trunk:get(2));
    -- print (seq_trunk:get(2).weight:size());

    -- make the combination conv unit weights
    -- seq_trunk:get(2).weight=torch.ones(seq_trunk:get(2).weight:size())/seq_trunk:get(2).nInputPlane;
    -- seq_trunk:get(2).bias=torch.zeros(seq_trunk:get(2).bias:size());
    
    seq:add(seq_trunk);



    local score_branch=nn.Sequential();
    score_branch:add(nn.Linear(512,1024));
    score_branch:add(nn.ReLU());
    score_branch:add(nn.Dropout(0.5));
    score_branch:add(nn.Linear(1024,1));
    
    score_branch = makeXavierGaussian(score_branch)

    local seg_branch=nn.Sequential();
    seg_branch:add(nn.Linear(512,40*40));
    seg_branch:add(nn.View(1,40,40));
    seg_branch:add(nn.SpatialUpSamplingNearest(4));
    
    seg_branch = makeXavierGaussian(seg_branch)

    local split_net=nn.ConcatTable();
    split_net:add(score_branch);
    split_net:add(seg_branch);

    local total_net = nn.Sequential();
    total_net:add(seq);
    total_net:add(split_net);

    return total_net
end

function getLossSeg(x_seg,gt_mask)
	local loss=torch.mul(gt_mask,-1)
	-- print(loss:size())
	-- print(gt_mask:size())
	loss:cmul(x_seg)
	loss:exp();
	loss:add(1);
	loss:log();
    loss:div(40*40);
	loss=loss:sum()
	return loss;
end

function getLossScore(x_score,gt_label)
    local loss=torch.mul(gt_label,-1)
    loss:cmul(x_score)
    loss:exp();
    loss:add(1);
    loss:log();
    loss:div(32);
    loss=loss:sum()
    return loss;
end

function getLossSegD(x_seg,gt_mask)
	local dloss=torch.mul(gt_mask,-1);
	dloss:cmul(x_seg);
	dloss:exp();
	local deno=torch.add(dloss,1);
	dloss:cmul(gt_mask);
	dloss:mul(-1);
	dloss:cdiv(deno);
	dloss:div(40*40);
	return dloss;
end

function getLossScoreD(x_score,gt_label)
	-- print (x_score)
	local dloss=torch.mul(gt_label,-1);
	dloss:cmul(x_score);
    dloss:exp();
    local deno=torch.add(dloss,1);
    dloss:cmul(gt_label);
    dloss:mul(-1);
    dloss:cdiv(deno);
    dloss:div(32);
    return dloss;
end

local fevalScore = function(x)
    if x ~= parameters then
        parameters:copy(x)
    end
    
    td:getTrainingDataToyScore();    
    local batch_inputs = td.training_set_score.data:cuda();
    local batch_inputs_flow = td.training_set_score.flo:cuda();
    local batch_targets = td.training_set_score.label:cuda();
    
    gradParameters:zero()
    
    local batch_outputs_mid = net:get(1):forward{batch_inputs,batch_inputs_flow}
    local batch_outputs = net:get(2):get(1):forward(batch_outputs_mid)

    local batch_loss = getLossScore(batch_outputs, batch_targets)
    local dloss_doutput = getLossScoreD(batch_outputs, batch_targets)
    local gradInputs=net:get(2):get(1):backward(batch_outputs_mid, dloss_doutput)
    net:get(1):backward(batch_inputs, gradInputs)
    return batch_loss, gradParameters
end


local fevalSeg = function(x)
    if x ~= parameters then
        parameters:copy(x)
    end
    
    td:getTrainingDataToy();    
    local batch_inputs = td.training_set_seg.data:cuda();
    local batch_inputs_flow = td.training_set_seg.flo:cuda();
    local batch_targets = td.training_set_seg.label:cuda();

    gradParameters:zero()
    local batch_outputs_mid = net:get(1):forward{batch_inputs,batch_inputs_flow}
    local batch_outputs = net:get(2):get(2):forward(batch_outputs_mid)
    local batch_loss = getLossSeg(batch_outputs, batch_targets)
    local dloss_doutput = getLossSegD(batch_outputs, batch_targets)
    
    local gradInputs=net:get(2):get(2):backward(batch_outputs_mid, dloss_doutput):clone()
    net:get(1):backward(batch_inputs, gradInputs)
    return batch_loss, gradParameters
end

function main(params) 
    local torch_file='/disk2/januaryExperiments/vgg_16/vgg16_onlyConv.dat';
    local deep_prop_file=params.model
    local out_dir=params.outDir
    
    if params.saveModel then 
        print ('saving model to ',deep_prop_file);
        net=makeMyNetwork(torch_file)
        torch.save(deep_prop_file,net);
    end

    local pos_file= params.pos_file;
    local neg_file= params.neg_file;
    
    local val_pos_file= params.pos_val_file
    local val_neg_file= params.neg_val_file
    
    paths.mkdir(out_dir);
    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    

    local out_file_net=out_dir_final..'/'..'model_all_final.dat';

    local out_file_loss_seg=out_dir_final..'/'..'loss_all_final_seg.npy';
    local out_file_loss_score=out_dir_final..'/'..'loss_all_final_score.npy';

    local out_file_intermediate_pre = out_dir_intermediate..'/'..'model_all_';
    local out_file_loss_intermediate_pre = out_dir_intermediate..'/'..'loss_all_';

    local out_file_loss_plot_intermediate_score=out_dir_intermediate..'/'..'loss_iter_score.png';
    local out_file_loss_plot_final_score=out_dir_final..'/'..'loss_iter_score.png';

    local out_file_loss_plot_intermediate_seg=out_dir_intermediate..'/'..'loss_iter_seg.png';
    local out_file_loss_plot_final_seg=out_dir_final..'/'..'loss_iter_seg.png';


    print (params);

    local opt = {}         
    opt.optimization = 'sgd'
    opt.batch_size=params.batchSize;
    opt.learningRate=params.learningRate;
    opt.testAfter=params.testAfter;
    opt.momentum=0.9
    opt.weightDecay=0.00005;
    opt.iterations=params.iterations;
    opt.saveAfter=params.saveAfter;
    opt.dispAfter=params.dispAfter;
    cutorch.setDevice(params.gpu);

    local optimState       
    local optimMethod      

    optimState = {
        learningRate = opt.learningRate,
        weightDecay = opt.weightDecay,
        momentum = opt.momentum,
    }

    optimMethod = optim.sgd


    print ('loading network');

    net = torch.load(deep_prop_file);
    print (torch.min(net:get(1):get(2):get(3).weight),torch.min(net:get(1):get(2):get(3).weight));

    print ('done loading network');
    print (net);

    print ('making cuda');
    net = net:cuda();
    print ('done');

    print ('loading params');
    parameters, gradParameters = net:getParameters()
    print ('loading done');
    print (optimState)
    td=data({file_path_positive=pos_file,file_path_negative=neg_file});
    vd=data({file_path_positive=val_pos_file,file_path_negative=val_neg_file});

    local counter = 0

    local losses_seg = {};
    local losses_score = {};
    local val_losses_seg = {};
    local val_losses_score = {};

    for i=1,opt.iterations do
        
        if i%2==0 then
            local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
            losses_score[#losses_score + 1] = minibatch_loss[1] -- append the new loss        
        else
            local _, minibatch_loss = optimMethod(fevalSeg,parameters, optimState)
            losses_seg[#losses_seg + 1] = minibatch_loss[1] -- append the new loss

        end

        if i%opt.dispAfter==0 then
            print(string.format("minibatches processed: %6s, loss seg = %6.6f, loss score = %6.6f", i, 
                losses_seg[#losses_seg],losses_score[#losses_score]))

            local str_seg=''..losses_seg[#losses_seg];
            local str_score=''..losses_score[#losses_seg];

            if str_seg=='nan' or str_score=='nan' then
                print('QUITTING');
                break;
            end
        end


        if i%opt.testAfter==0 and opt.testAfter>0 then 

            net:evaluate();
            
            vd:getTrainingDataToyScore();
            local batch_inputs = vd.training_set_score.data:cuda();
            local batch_inputs_flow =vd.training_set_score.flo:cuda();
            local batch_targets = vd.training_set_score.label:cuda();

            local out_results=net:forward{batch_inputs,batch_inputs_flow}
            local score_loss_val = getLossScore(out_results[1], batch_targets)

            vd:getTrainingDataToy();
            
            batch_inputs = vd.training_set_seg.data:cuda();
            batch_inputs_flow =vd.training_set_score.flo:cuda();
            batch_targets = vd.training_set_seg.label:cuda();
            
            out_results=net:forward{batch_inputs,batch_inputs_flow}
            local seg_loss_val = getLossSeg(out_results[2],batch_targets);
            
            val_losses_seg[#val_losses_seg+1]=seg_loss_val;
            val_losses_score[#val_losses_score+1]=score_loss_val;

            net:training();

            print(string.format("minibatches processed: %6s, val loss seg = %6.6f, val loss score = %6.6f", i, 
                val_losses_seg[#val_losses_seg],val_losses_score[#val_losses_score]))
        end


        -- check if model needs to be saved. save it.
        -- also save losses
        if i%opt.saveAfter==0 then
            local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
            torch.save(out_file_intermediate,net);
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_score.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses_score))
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_seg.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses_seg))

            if opt.testAfter>0 then 
                local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val_score.npy';
                npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses_score))
                local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'_val_seg.npy';
                npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(val_losses_seg))
            end
        end

    end

    -- save final model
    torch.save(out_file_net,net);
    npy4th.savenpy(out_file_loss_seg, torch.Tensor(losses_seg))
    npy4th.savenpy(out_file_loss_score, torch.Tensor(losses_score))

end

cmd = torch.CmdLine()
cmd:text()
cmd:text('Debug Train Network With Flow')
cmd:text()
cmd:text('Options')
cmd:option('-model','','model to load')
cmd:option('-outDir','','directory to write output');
cmd:option('-iterations',100000,'num of iterations to run');
cmd:option('-learningRate',0.001,'starting learning rate to use');
cmd:option('-saveAfter',5000,'num of iterations after which to save model');
cmd:option('-batchSize',32,'batch size');
cmd:option('-testAfter',0,'num iterations after which to get validation loss');
cmd:option('-dispAfter',4,'num iterations after which to display training loss');
cmd:option('-pos_file','/disk2/aprilExperiments/positives_160.txt','pos train data file');
cmd:option('-neg_file','/disk2/marchExperiments/deep_proposals/negatives.txt','neg train data file');
cmd:option ('-pos_val_file','/disk2/aprilExperiments/positives_160.txt','pos val data file');
cmd:option ('-neg_val_file','/disk2/marchExperiments/deep_proposals/negatives.txt','neg val data file');
cmd:option('-saveModel',false,'save the model to load')
cmd:option('-gpu',1,'gpu to run the training on');
cmd:text()

params = cmd:parse(arg)
main(params);

